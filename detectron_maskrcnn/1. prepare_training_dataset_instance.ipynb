{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83fde5-8a95-47da-9b6c-cd5f32a9396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in anaconda prompt:\n",
    "#!conda activate detectron_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c49d24-10f0-40e6-b8fc-544329c07458",
   "metadata": {},
   "outputs": [],
   "source": [
    "Assumption: \n",
    "# you have the large orthophoto and digitized labels over orthophoto in labelme tool. each solar has different ids (no need to do anything they will have by default)\n",
    "# saved the file. this will give the json file. You need to convert this json file to coco format json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295740f-567a-400f-92f8-c7642adba347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labelme json to coco json. Use anaconda prompt after activating detectron_env.This will generate the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0febab06-d422-4a2d-ae8e-b14ea2c03154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goto path where you want to save coco json: C:\\Users\\ROG\\Documents\\Termatics\\segmentation\\detectron_maskrcnn\\jsons\\coco_json\n",
    "#labelme2coco \"C:\\Users\\ROG\\Documents\\Termatics\\segmentation\\detectron_maskrcnn\\jsons\\labelme_json\"\n",
    "#Note: make sure you only have one json in the folder json. Otherwise it will also try to convert other files even if they are not json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a74fa-2e47-4c8d-892c-2272dc924361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, you have the coco json and the orthophoto (one big image and coco json (labels) for that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b9c2d-32d3-4e46-aa8d-d7532e825b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split big image into smaller tiles and generate another json for those smaller tiles in coco format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc062e3-467c-4764-a6ac-f0aa289d406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 25.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIG ---\n",
    "tile_size = 512\n",
    "overlap = 0\n",
    "image_path = r'ortho/solar_pv_True_Ortho_Clip.tif'\n",
    "coco_json_path = r'jsons\\coco_json\\runs\\labelme2coco\\dataset.json'\n",
    "output_dir = \"training_dataset_generated\"\n",
    "\n",
    "# Create folders\n",
    "img_out_dir = os.path.join(output_dir, \"images\")\n",
    "os.makedirs(img_out_dir, exist_ok=True)\n",
    "\n",
    "# --- Load original image ---\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    raise ValueError(f\"Image not found at path: {image_path}\")\n",
    "H, W, _ = image.shape\n",
    "\n",
    "# --- Load COCO annotations ---\n",
    "with open(coco_json_path) as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "categories = coco[\"categories\"]\n",
    "anns = coco[\"annotations\"]\n",
    "img_info = coco[\"images\"][0]\n",
    "img_id = img_info[\"id\"]\n",
    "\n",
    "# --- Create new COCO structure ---\n",
    "new_coco = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": categories,\n",
    "}\n",
    "ann_id = 0\n",
    "tile_id = 0\n",
    "\n",
    "# --- Slide and create tiles ---\n",
    "for y in tqdm(range(0, H, tile_size - overlap)):\n",
    "    for x in range(0, W, tile_size - overlap):\n",
    "        x1, y1 = x, y\n",
    "        x2, y2 = min(x + tile_size, W), min(y + tile_size, H)\n",
    "        tile = image[y1:y2, x1:x2]\n",
    "\n",
    "        tile_annotations = []\n",
    "\n",
    "        for ann in anns:\n",
    "            if ann[\"image_id\"] != img_id:\n",
    "                continue\n",
    "\n",
    "            seg = ann[\"segmentation\"][0]\n",
    "            poly = np.array(seg).reshape(-1, 2)\n",
    "\n",
    "            if np.any((poly[:, 0] >= x1) & (poly[:, 0] <= x2) &\n",
    "                      (poly[:, 1] >= y1) & (poly[:, 1] <= y2)):\n",
    "\n",
    "                new_poly = (poly - [x1, y1]).reshape(-1).tolist()\n",
    "                xmin, ymin = np.min(poly, axis=0) - [x1, y1]\n",
    "                xmax, ymax = np.max(poly, axis=0) - [x1, y1]\n",
    "                area = (xmax - xmin) * (ymax - ymin)\n",
    "\n",
    "                tile_annotations.append({\n",
    "                    \"id\": int(ann_id),\n",
    "                    \"image_id\": int(tile_id),\n",
    "                    \"category_id\": int(ann[\"category_id\"]),\n",
    "                    \"segmentation\": [list(map(float, new_poly))],\n",
    "                    \"bbox\": [float(xmin), float(ymin), float(xmax - xmin), float(ymax - ymin)],\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"area\": float(area)\n",
    "                })\n",
    "                ann_id += 1\n",
    "\n",
    "        if tile_annotations:\n",
    "            tile_filename = f\"tile_{tile_id}.jpg\"\n",
    "            tile_path = os.path.join(img_out_dir, tile_filename)\n",
    "            cv2.imwrite(tile_path, tile)\n",
    "\n",
    "            new_coco[\"images\"].append({\n",
    "                \"id\": int(tile_id),\n",
    "                \"width\": int(x2 - x1),\n",
    "                \"height\": int(y2 - y1),\n",
    "                \"file_name\": tile_filename\n",
    "            })\n",
    "\n",
    "            new_coco[\"annotations\"].extend(tile_annotations)\n",
    "            tile_id += 1\n",
    "\n",
    "# --- Save new COCO JSON ---\n",
    "with open(os.path.join(output_dir, \"annotations.json\"), \"w\") as f:\n",
    "    json.dump(new_coco, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c818ef-79bd-4262-9a53-2645901d90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and validation sets. It will only split json files which detectron by default expects. Images are splitted in next code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3978a912-140f-4b46-b79a-b4a4c2e7967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Train: 111 images, Val: 28 images\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "input_json_path = \"training_dataset_generated/annotations.json\"\n",
    "output_dir = \"training_dataset_generated/training_sets/annotations\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Load COCO JSON ---\n",
    "with open(input_json_path, 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# --- Shuffle and split images ---\n",
    "random.seed(42)\n",
    "images = coco[\"images\"]\n",
    "random.shuffle(images)\n",
    "\n",
    "split_index = int(0.8 * len(images))\n",
    "train_images = images[:split_index]\n",
    "val_images = images[split_index:]\n",
    "\n",
    "# --- Map image_id to images ---\n",
    "train_ids = {img[\"id\"] for img in train_images}\n",
    "val_ids = {img[\"id\"] for img in val_images}\n",
    "\n",
    "# --- Split annotations ---\n",
    "train_anns = [ann for ann in coco[\"annotations\"] if ann[\"image_id\"] in train_ids]\n",
    "val_anns = [ann for ann in coco[\"annotations\"] if ann[\"image_id\"] in val_ids]\n",
    "\n",
    "# --- Build final COCO JSONs ---\n",
    "train_coco = {\n",
    "    \"images\": train_images,\n",
    "    \"annotations\": train_anns,\n",
    "    \"categories\": coco[\"categories\"]\n",
    "}\n",
    "val_coco = {\n",
    "    \"images\": val_images,\n",
    "    \"annotations\": val_anns,\n",
    "    \"categories\": coco[\"categories\"]\n",
    "}\n",
    "\n",
    "# --- Save output ---\n",
    "with open(os.path.join(output_dir, \"instances_train.json\"), \"w\") as f:\n",
    "    json.dump(train_coco, f, indent=2)\n",
    "\n",
    "with open(os.path.join(output_dir, \"instances_val.json\"), \"w\") as f:\n",
    "    json.dump(val_coco, f, indent=2)\n",
    "\n",
    "print(f\"Done! Train: {len(train_images)} images, Val: {len(val_images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05080bd0-a742-4c84-956c-4a8fcdc06f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the images into train and valid folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc1383b-d021-412b-a347-eae8ad379e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images moved to train_images/ and val_images/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "img_dir = \"training_dataset_generated/images\"\n",
    "ann_dir = \"training_dataset_generated/training_sets/annotations\"\n",
    "train_dir = \"training_dataset_generated/training_sets/train_images\"\n",
    "val_dir = \"training_dataset_generated/training_sets/val_images\"\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Load train and val JSONs\n",
    "with open(os.path.join(ann_dir, \"instances_train.json\")) as f:\n",
    "    train_json = json.load(f)\n",
    "with open(os.path.join(ann_dir, \"instances_val.json\")) as f:\n",
    "    val_json = json.load(f)\n",
    "\n",
    "# Move images\n",
    "for img in train_json[\"images\"]:\n",
    "    src = os.path.join(img_dir, img[\"file_name\"])\n",
    "    dst = os.path.join(train_dir, img[\"file_name\"])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "for img in val_json[\"images\"]:\n",
    "    src = os.path.join(img_dir, img[\"file_name\"])\n",
    "    dst = os.path.join(val_dir, img[\"file_name\"])\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "print(\"Images moved to train_images/ and val_images/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce09ed7-a00b-4735-b58e-9efb3ebcdb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Detectron2)",
   "language": "python",
   "name": "detectron_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
